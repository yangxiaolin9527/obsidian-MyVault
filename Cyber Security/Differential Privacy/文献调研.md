> 关于差分隐私在网络安全中的应用，存在哪些问题，当前方法的最新进展以及存在的缺陷

主要应用领域：security of ai，针对于安全要素中的隐私性，主要分为两类
1. 数据安全
2. 模型安全

## 应用场景
1. 联邦学习
	分布式学习主要用来应对两种场景下的机器学习
	* 多节点并行学习，充分利用节点的计算能力和资源，加速训练。通常需要于大数据相关技术结合
	* 数据分布不在本地，各节点协作完成模型训练，而无需共享原始数据。
    FL属于后者，天生具有隐私保护属性，根据学习模式的不同，可以分为
    * Horizontal FL: a.k.a. sample-based FL.数据在各节点间水平分布，具有相同的特征。例如，如果两家医院使用 HFL 进行合作，它们可能都有关于不同患者的数据，但具有相同类型的医疗记录（例如，年龄、性别、病史）。
    * Vertical FL: a.k.a. feature-based FL.特征在各节点间垂直分布，各节点各包含同一数据的部分特征。例如，一方可能拥有关于一组客户的人口统计数据（年龄、性别），而另一方则拥有针对同一组客户的交易数据（购买历史记录）。
    * 上述两种的混合。
    FL主要存在以下几个问题：
    * 通信效率
    * 数据的异构性（non-iid）
    * 隐私保护问题（虽然没有直接交换原始数据，但是传输模型参数和梯度也同样收到攻击）
    * 系统的异构性（不同节点的算力不同）
   
   个性化联邦学习（Personalized-FL）,考虑到non-iid的问题，PFL并非为所有节点训练一个global的模型，而是为每个节点个性化训练模型。
    * global模型作为基座预训练+节点微调
    * 多任务FL
    * **联邦元学习**，训练一个用于快速适应的global模型
    * 聚类联邦学习，具有相似分布数据的节点作为一个簇，为每一个簇训练个性化模型
    * 个性正则化，每个节点训练时，损失函数增加一项度量本地模型和全局模型差距的惩罚项。(类似思想可见iMAML)
    * Mixed FL，全局模型和本地模型合作做出预测
2. 机器学习（深度学习）
   与DP相关的两类主要攻击类型
   * Membership Inference Attack. 属于record-level攻击，目的是确定是否数据集中包含特定的数据记录。
   * Attribute Inference Attack. 也称为模型逆向攻击属于Attribute-level攻击，目的是推断出数据集中的隐私信息。
   代表paper：DPSGD
   * 理论创新：修改噪声，着眼于DP的局限“trade off between privacy and accuracy”，保证同一level的privacy preserving下，具有更高的accuracy
   * 针对隐私预算的上下界，进行改进。研究表明，DPSGD的上界过于严苛，实际的攻击者往往无法获得充足的攻击信息，达不到理论的上界。关注隐私预算的下界往往更加具有实际意义。设计攻击使得下界与上界足够接近是未来的一个研究方向
	   * 针对神经网络进行优化，尽可能减小对NN的影响。例如，与training epoch无关，自适应噪声。


# PFL+Meta-L+DP?

1. FL 开山之作：FedAvg

2. [Personalized federated learning with differential privacy and convergence guarantee](https://scholar.google.com/scholar?cluster=10947438302885564061&hl=zh-CN&as_sdt=2005&sciodt=0,5) : RDP-FL（+MAML） 框架 based DP-FL(DPSGD)
   `Specifically, we first design a privacy budget allocation scheme for inner and outer update stages based on the Rényi DP composition theory. Then, we develop two convergence bounds for the proposed DP-PFL framework under convex and non-convex loss function assumptions, respectively. Our developed convergence bounds reveal that 1) there is an optimal size of the DP-PFL model that can achieve the best convergence performance for a given privacy level, and 2) there is an optimal tradeoff among the number of communication rounds, convergence performance and privacy budget. Evaluations on various real-life datasets demonstrate that our theoretical results are consistent with experimental results. The derived theoretical results can guide the design of various DP-PFL algorithms with configurable tradeoff requirements on the convergence performance and privacy levels.`
3. per-FedAvg: 
   Based on FedAvg+MAML（FO-MAML, HF-MAML）
   `In this paper, we study a personalized variant of the federated learning in which our goal is to find an initial shared model that current or new users can easily adapt to their local dataset by performing one or a few steps of gradient descent with respect to their own data. This approach keeps all the benefits of the federated learning architecture, and, by structure, leads to a more personalized model for each user. We show this problem can be studied within the Model-Agnostic Meta-Learning (MAML) framework. Inspired by this connection, we study a personalized variant of the well-known Federated Averaging algorithm and evaluate its performance in terms of gradient norm for non-convex loss functions. Further, we characterize how this performance is affected by the closeness of underlying distributions of user data, measured in terms of distribution distances such as Total Variation and 1-Wasserstein metric.`
4. CAFeMe context-aware federated learning based on meta-learning
   `We propose a novel framework that combines federated learning with meta-learning techniques to enhance both efficiency and generalization capabilities. Our approach introduces a federated modulator that learns contextual information from data batches and uses this knowledge to generate modulation parameters. These parameters dynamically adjust the activations of a base model, which operates using a MAML-based approach for model personalization. Experimental results across diverse datasets highlight the improvements in convergence speed and model performance compared to existing federated learning approaches. These findings highlight the potential of incorporating contextual information and meta-learning techniques into federated learning, paving the way for advancements in distributed machine learning paradigms.`


   