> 评估LLM reasoning的能力通常是通过模型**解决数学相关问题的能力**来评估

#evaluation 

![[Pasted image 20250714174101.png]]

# Benchmark

![[Pasted image 20250714174510.png]]

## Human Evaluation

![[Pasted image 20250714174646.png]]

### Elo Score
> “Elo Score”即埃洛评分，是一种用于**衡量竞技比赛中选手相对实力的评分系统**。它基于比赛结果，通过数学模型来动态调整选手的分数。比如在国际象棋比赛中，两名选手对弈，若实力评分高的选手获胜，其评分提升幅度相对较小，而实力评分低的选手若获胜，其评分提升幅度则较大；若实力评分高的选手输了，评分下降幅度会较大。这个系统广泛应用于各类竞技领域，像棋类、电竞等比赛，以较为科学地反映选手或队伍在不同阶段的实力水平。 

![[Pasted image 20250714175303.png]]

## Goodhart’s law in LLM's evaluation
Goodhart定律的核心思想是：当一个度量成为目标时，它就不再是一个好的度量。在评估大型语言模型（LLM）时，这个定律同样适用。

在LLM的评估中，我们通常会使用某些指标来衡量模型的性能，比如准确率、困惑度（perplexity）等。这些指标最初是为了客观地评估模型的质量而设计的。然而，**当这些指标成为优化目标时，模型可能会过度拟合这些指标，而忽视了实际应用中的真实需求。**

例如，假设我们使用困惑度来评估一个语言模型的质量。困惑度是一个衡量模型预测下一个词的能力的指标，定义为：
$$
PPL = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i)}
$$

其中，$P(w_i)$是模型预测词$w_i$的概率，$N$是词的总数。虽然较低的困惑度通常表示模型在训练数据上的表现较好，但如果模型过于关注降低困惑度，它可能会在训练数据上表现得很好，却在真实场景中表现不佳。

Goodhart定律提醒我们，在评估和优化LLM时，不能仅仅依赖单一的指标。我们需要综合考虑多种因素，并确保模型在实际应用中能够有效满足用户需求，而不仅仅是在某些指标上表现优异。这意味着**在设计评估标准时，我们需要不断调整和更新，以避免模型过度拟合某些特定的度量**。