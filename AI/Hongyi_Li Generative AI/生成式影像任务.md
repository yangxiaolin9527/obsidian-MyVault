# Introduction
## Basic tasks
![[Pasted image 20250707161853.png]]

![[Pasted image 20250707162027.png]]
![[Pasted image 20250707162241.png]]

## How does AI see image/video
![[Pasted image 20250707162903.png]]

# How to train text2image
## data
![[Pasted image 20250707164251.png]]

## NAR and Transformer
> Transformer中Attention机制可以缓解NAR带来的多模态问题，但是并不能完全解决。

![[Pasted image 20250707165413.png]]

## Evaluation
> human's evaluation is better but expensive

### CLIP score fro image generation
#clip
![[Pasted image 20250707165714.png]]
## personalization
> 一图胜千言

![[Pasted image 20250707170012.png]]

# How to train text2video
## challenge
![[Pasted image 20250707170323.png]]

## 三种Attention机制
![[Pasted image 20250707170520.png]]

### 伪3D注意力机制
![[Pasted image 20250707170654.png]]

## 迭代AR技术
![[Pasted image 20250707170843.png]]

# Algorithms
## Challenge
> 多模态问题

![[Pasted image 20250707171547.png]]

## Idea
> 在训练生成模型时，尽可能将prompt精细化，增加更多的标注
> 人力标注？浪费资源，费时费力

==如何训练一个可以从unlabelled（或者few-label）的数据中学习精细化label的网络==
其中，学习到的**低维标注embedding**，**人认不认识不重要，只要网络认识即可**
#auto-encoder 
![[Pasted image 20250707172138.png]]

#conditional_GAN 
==测试时，没有给定的精确描述，就使用随机的vector==
![[Pasted image 20250707172716.png]]

## VAE and Flow-based Model
![[Pasted image 20250707172924.png]]

### noise is important
#流形假设 
![[Pasted image 20250707173011.png]]

## Diffusion
### denoise
![[Pasted image 20250707173342.png]]

### train with transformer
![[Pasted image 20250707173457.png]]

### Comparison
![[Pasted image 20250707173637.png]]

## GAN
#GAN 
![[Pasted image 20250707182122.png]]


