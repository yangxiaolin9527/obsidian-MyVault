# 摘要
#k-means #em
本次探讨将从最直观的聚类算法K-Means出发，分析其作为一种“硬”聚类方法的局限性。为了克服这些局限，我们引入概率思想，将聚类问题重新表述为一个基于概率的“软”聚类问题，从而自然地引出**隐变量（Latent Variable）的概念和混合高斯模型（Gaussian Mixture Model, GMM）**。然而，直接对GMM的对数似然函数进行最大化会遇到解析困难，这正是我们需要**期望最大化（Expectation-Maximization, EM）算法的原因。我们将详细拆解EM算法的两个步骤（E步和M步），并展示它如何迭代求解GMM的参数。最后，我们将深入分析K-Means实际上是EM算法在特定假设下的一种特例，并探讨这两种算法背后的核心思想（如隐变量、迭代优化）如何在变分自编码器（VAE）、注意力机制（Attention）等现代深度学习模型**中得到体现和发展。

---
# 1. K-Means算法：确定性的硬聚类
K-Means的目标是将数据集$X={x_1​,x_2​,...,x_N​}$划分为$K$个互不相交的簇 $C={C_1​,C_2​,...,C_K​}$，并使得簇内数据点的方差（或距离平方和）最小。
## 1.1 目标函数
K-Means的优化目标是**最小化所有簇的簇内平方和（Within-Cluster Sum of Squares, WCSS）**，其目标函数$J$定义为：
$$
J=\sum_{k=1}^{K} \sum_{x_i \in C_k} \|x_i - \mu_k\|^2
$$
其中：
- $K$ 是预先设定的簇的数量。
- $C_k$ 是第$k$个簇的集合。
- $μ_k$ 是第$k$个簇的**质心（Centroid）**，即该簇内所有数据点的均值。
- $∣∣x_i−μ_k∣∣^2$ 是样本 $x_i$ 到其所属簇质心 $μ_k$ 的欧氏距离的平方。
## 1.2 算法流程
这是一个NP-hard问题，但可以通过一个启发式的迭代算法来求解，这个过程可以被看作是两个步骤的交替执行：
1. 分配步骤（Assignment Step）: 对于每个数据点$x_i$​，将其分配给离它最近的质心所代表的簇。**这是一种硬分配（Hard Assignment），因为每个点在这一步被唯一地、确定地分到一个簇中**。
    $$
    C_k^{(t)} = { x_i : ||x_i - \mu_k^{(t)}||^2 \le ||x_i - \mu_j^{(t)}||^2 \quad \forall j, 1 \le j \le K }
    $$
    注解：上标 (t) 代表第 t 次迭代。
2. 更新步骤（Update Step）: 对于每个簇 $C_k$，重新计算其质心 $μ_k$，即该簇内所有数据点的新均值。
    $$
    
    \mu_k^{(t+1)} = \frac{1}{|C_k^{(t)}|} \sum_{x_i \in C_k^{(t)}} x_i
    
    $$
算法从随机初始化 $K$ 个质心开始，交替执行这两个步骤，**直到质心的位置不再发生显著变化或达到最大迭代次数为止**。

**核心思想**：K-Means是一种非常直观的算法，它假设簇的形状是**球形**的（因为使用欧氏距离），并且每个数据点都100%地属于某一个簇。
# 2. 从硬聚类到软聚类：引入概率与隐变量
K-Means的硬分配带来了两个问题：
1. **对簇形状的假设过强**：它隐含地假设所有簇都是球形的，且大小相近。对于非球形（如椭圆形）的簇，效果不佳。
2. **分配的绝对性**：对于处于簇边界的数据点，强制将其分到某一类是不合理的。更自然的方式是描述它属于各个簇的**概率**。
为了解决这些问题，我们引入概率框架。
## 2.1 生成式观点与隐变量
我们不妨换个角度思考数据是如何产生的（即**生成式观点**）：
假设数据是由 $K$ 个不同的分布（或称为“组件”）混合而成的。每个数据点 $x_i$ 的生成过程如下：
1. 首先，根据一个**类别先验概率** $\pi_k$ (其中 $\sum_{k=1}^{K}\pi_k=1$)，选择一个组件 $k$。
2. 然后，**从该组件所对应的概率分布 $p(x|component=k)$ 中采样**，生成数据点 $x_i$。
在这个过程中，我们能**观测到的是数据点** $x_i$，但我们**无法观测**到它究竟是由哪个组件生成的。这个未知的、无法观测的变量，我们称之为**隐变量（Latent Variable）**。
我们引入一个 $K$ 维的二元随机变量 $z$，采用 **1-of-K** 表示法。如果 $x_i$ 来自第 $k$ 个组件，则 $z_i$ 的第 $k$ 维 $z_{ik}=1$，其他维度为0。因此，$p(z_{ik}=1)=\pi_k$。
## 2.2 混合高斯模型 (GMM)
如果我们假设每个组件的概率分布是**高斯分布（Gaussian Distribution）**，那么这个模型就是**混合高斯模型（Gaussian Mixture Model, GMM）**。
在GMM中，第 $k$ 个组件是一个均值为 $μ_k$、协方差矩阵为 $Σ_k$ 的高斯分布，其概率密度函数为 $N(x|μ_k,Σ_k)$。
根据全概率公式，任意一个数据点 $x$ 的边缘概率分布为所有组件的加权和：
$$
p(x|\theta)=\sum_{k=1}^{K}p(z_k=1)p(x|z_k=1)=\sum_{k=1}^{K}\pi_kN(x|μ_k,Σ_k)
$$
这里的模型参数集合为 $\theta=\{\pi_k,μ_k,Σ_k\}_{k=1}^{K}$。
**注解：高斯分布**
- $N(x|\mu,\Sigma)=(2\pi)^{D/2}|\Sigma|^{-1/2}\exp{(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}$
- 其中 $D$ 是数据维度，$|\Sigma|$ 是协方差矩阵的行列式。
- ==协方差矩阵 $\Sigma_k$ 能够描述簇的形状，可以是球形（对角且元素相等）、对角（轴对齐的椭球）或全矩阵（任意方向的椭球），这使得GMM比K-Means能描述更复杂的簇结构==。
# 3. EM算法：求解隐变量模型的钥匙
有了模型 $p(x|\theta)$，我们的目标是找到一组参数 $\theta$ 来**最大化对数似然函数（Log-Likelihood Function）**，以最好地拟合观测到的数据 $X=\{x_1,...,x_N\}$。
$$
L(\theta)=\log p(X|\theta)=\log \left( \prod_{i=1}^{N} p(x_i|\theta) \right) = \sum_{i=1}^{N} \log \left( \sum_{k=1}^{K} \pi_k N(x_i|\mu_k,\Sigma_k) \right)
$$
**核心困难**：我们遇到了“**和的对数**”（log of a sum）问题。直接对 $L(\theta)$ 求导并令其为零，会发现参数（如 $\mu_k$、$\Sigma_k$）的解是相互耦合的，无法得到一个封闭的解析解。
这就是EM算法登场的舞台。**EM算法是一种迭代方法，专门用于求解含有隐变量的概率模型的最大似然估计问题**。
## 3.1 EM算法的核心思想与流程
EM算法巧妙地避开了直接优化 $L(\theta)$ 的困难。它通过引入一个辅助函数（Q函数），**该函数是完整数据对数似然的期望，并且是原始对数似然函数 $L(\theta)$ 的一个下界**。通过迭代地最大化这个下界，来间接最大化 $L(\theta)$。
算法交替执行以下两个步骤：
1. E步（Expectation Step）：计算“期望”。在这一步，我们**固定当前的参数 $\theta^{(t)}$，计算隐变量 $z$ 的后验概率分布**。对于GMM，这意味着计算**每个数据点 $x_i$ 来自于每个高斯组件 $k$ 的后验概率或责任（Responsibility）**。
    $$
    \gamma(z_{ik})^{(t)} = p(z_k=1 | x_i, \theta^{(t)}) = \frac{p(x_i | z_k=1, \theta^{(t)}) p(z_k=1 | \theta^{(t)})}{\sum_{j=1}^{K} p(x_i | z_j=1, \theta^{(t)}) p(z_j=1 | \theta^{(t)})}
    $$
    
    代入GMM的具体形式：
    $$
    \gamma(z_{ik})^{(t)} = \frac{\pi_k^{(t)} \mathcal{N}(x_i | \mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_{j=1}^{K} \pi_j^{(t)} \mathcal{N}(x_i | \mu_j^{(t)}, \Sigma_j^{(t)})}
    $$

    **注解**：$\gamma(z_{ik})^{(t)}$ 表示在当前参数下，第 $i$ 个数据点由第 $k$ 个组件生成的概率。这是一个 0 到 1 之间的值，**代表了软分配（Soft Assignment）。所有组件对数据点 $x_i$ 的责任之和为1**，即 $\sum_{k=1}^{K} \gamma(z_{ik})^{(t)}=1$。
2. M步（Maximization Step）：最大化“期望”。在这一步，我们使用E步计算出的责任 $\gamma(z_{ik})^{(t)}$ 来更新模型参数 $\theta$，以==最大化完整数据对数似然的期望（即 Q 函数）==。更新后的参数记为 $\theta^{(t+1)}$。
    GMM的参数更新公式如下：
    $$
    \mu_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma(z_{ik})^{(t)} x_i}{\sum_{i=1}^{N} \gamma(z_{ik})^{(t)}}
    $$ 
    $$
    \Sigma_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma(z_{ik})^{(t)} (x_i - \mu_k^{(t+1)})(x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^{N} \gamma(z_{ik})^{(t)}}
    $$
    $$
    \pi_k^{(t+1)} = \frac{\sum_{i=1}^{N} \gamma(z_{ik})^{(t)}}{N}
    $$

    注解：观察这些更新公式。新质心 $\mu_k^{(t+1)}$ 是所有数据点的加权平均，权重就是每个点对该簇的责任 $\gamma(z_{ik})^{(t)}$。**这与K-Means中只使用簇内点的（等权重）平均形成了鲜明对比**。$\pi_k^{(t+1)}$ 的更新也直观，它等于所有数据点对簇 $k$ 的平均责任。

EM算法保证了每次迭代后，对数似然函数 $L(\theta)$ 的值是单调不减的，最终会收敛到局部最优解。
# 4. K-Means与EM/GMM的内在联系与区别
## 4.1 联系：K-Means是EM算法的特例
K-Means可以被看作是EM算法在GMM框架下的一种**硬化（hard version）**的极限情况。这个极限情况的假设是：
1. 所有高斯组件的协方差矩阵是**各向同性（Isotropic）且相等的**，即 $\Sigma_k=\sigma^2I$，其中 $I$ 是单位矩阵。
2. 这个方差 $\sigma^2$ 趋近于零（$\sigma^2\to0$）。
让我们看看在这个极限下，EM算法会发生什么：
- **E步的变化**：当 $\sigma^2\to0$ 时，高斯分布 $N(x_i|\mu_k,\sigma^2I)$ 会变得极其尖锐。对于一个给定的 $x_i$，只有一个 $k$ 能使得距离 $\|x_i-\mu_k\|^2$ 最小，其对应的概率密度会趋向无穷大，而其他组件的概率密度则趋向零。**因此，后验概率 $\gamma(z_{ik})$ 会变成一个 0或1 的值**：
    - 如果 $\mu_k$ 是离 $x_i$ 最近的质心，则 $\gamma(z_{ik})\to1$。
    - 否则，$\gamma(z_{ik})\to0$。 这等价于K-Means的**硬分配**步骤。
- **M步的变化**：
    - **均值更新**：$\mu_k^{(t+1)}$ 的更新公式变为对那些 $\gamma(z_{ik})^{(t)}=1$ 的数据点求平均值，这与K-Means的质心更新公式完全相同。
    - **协方差和先验更新**：在这两个假设下，$\Sigma_k^{(t+1)}$ 和 $\pi_k^{(t+1)}$ 的更新变得无关紧要或简化。
因此，K-Means的迭代过程可以完美地对应于EM算法在特定约束下的迭代过程。
## 4.2 区别总结

| **特性**    | **K-Means**          | **GMM with EM**      |
| --------- | -------------------- | -------------------- |
| **分配方式**  | 硬分配（Hard Assignment） | 软分配（Soft Assignment） |
| **簇形状假设** | 球形（Spherical）        | 椭球形（Ellipsoidal），更灵活 |
| **模型基础**  | 基于距离/方差              | 基于概率模型（生成式模型）        |
| **核心变量**  | 簇分配（离散）              | 隐变量后验概率（连续）          |
| **目标函数**  | 最小化簇内平方和             | 最大化对数似然函数            |
| **复杂度**   | ==计算简单==，速度快         | 计算复杂，涉及协方差矩阵求逆       |
# 5. 在深度学习中的应用与演进
K-Means和EM算法背后的核心思想，特别是**隐变量建模**和**迭代优化**，在现代深度学习中得到了广泛的应用和升华。
## 5.1 变分自编码器 (Variational Autoencoder, VAE)
VAE是深度生成模型中的一个里程碑。它与GMM/EM有深刻的联系：
- **隐变量模型**：VAE假设观测数据 $x$ 是由一个低维的、连续的隐变量 $z$ 生成的，即 $p(x|z)$，这个过程**由一个解码器（Decoder）神经网络实现**。
- **棘手的后验**：与GMM类似，推断隐变量的后验 $p(z|x)$ 是非常困难的（intractable）。
- **EM思想的演进**：VAE没有直接使用EM，而是引入了**变分推断（Variational Inference）**。它使用另一个神经网络，即**编码器（Encoder）** $q(z|x)$，来近似真实的后验 $p(z|x)$。
    - **E步的类比**：编码器 $q(z|x)$ 的作用类似于E步，它根据输入 $x$ "推断"出隐变量 $z$ 的可能分布。
    - **M步的类比**：模型的训练目标是最大化**证据下界（Evidence Lower Bound, ELBO）**。这个优化过程会同时更新生成模型（解码器）和推断模型（编码器）的参数，类似于M步更新模型参数。
    - ==VAE可以看作是EM思想在深度学习和变分推断框架下的一个连续、可微、摊销化（amortized）的版本==。
## 5.2 注意力机制 (Attention Mechanism)
注意力机制，特别是**软注意力（Soft Attention）**，可以看作是软分配思想的应用。
- 在Seq2Seq模型或Transformer中，解码器在生成每个输出词时，会计算一个对输入序列所有位置的**注意力权重**分布。
- 这个权重分布 $\alpha_t$ 就类似于GMM中的责任 $\gamma(z_{ik})$。它是一个**软分配**，表示在当前步，应该“关注”输入序列中不同部分的程度。
- 这些权重被用来对输入序列的表示进行加权求和，生成一个上下文向量，这与G-M的M步中用责任对数据点进行加权求和来更新质心的思想异曲同工。
## 5.3 混合专家模型 (Mixture of Experts, MoE)
MoE是现代大型语言模型（如GPT-4）中用于扩大模型容量、同时控制计算成本的关键技术。
- MoE模型由一个**门控网络（Gating Network）**和多个**专家网络（Expert Networks）**组成。
- **对于每个输入，门控网络会输出一个概率分布，决定将这个输入以多大的权重分配给各个专家。这完全是GMM思想的直接体现**：
    - **门控网络** ⟺ **先验概率** $\pi_k$ 和**后验计算**（E步）。
    - **专家网络** ⟺ **高斯组件** $N(x|\mu_k,\Sigma_k)$。
- 最终的输出是所有专家网络输出的加权平均，权重就来自门控网络的软分配。
## 5.4 自监督学习中的聚类
一些先进的自监督学习方法（如DeepCluster, SwAV）直接将聚类思想融入训练流程：
- 它们交替进行两个步骤：
    1. **聚类特征**：将通过神经网络提取的数据特征进行K-Means聚类，得到每个样本的伪标签（Pseudo-label）。（**类E步**）
    2. **训练网络**：使用这些伪标签作为监督信号，来训练神经网络的参数，使其能够预测这些聚类分配。（**类M步**）
- 这种“聚类-训练”的迭代过程，完美复现了EM算法的核心循环，证明了这种古老而强大的思想在解决现代表征学习问题时依然充满活力。
# 总结
从K-Means到EM/GMM，我们完成了一次从确定性、基于距离的视角到概率性、基于生成模型的视角的转变。这一转变为处理更复杂的数据结构和不确定性提供了强大的数学工具。K-Means作为EM算法在特定假设下的简化形式，揭示了两者深刻的内在联系。而EM算法的核心思想——**通过迭代期望和最大化来解决含有隐变量的优化问题——已经超越了其最初的应用场景，演化并融入到深度学习的基石之中，成为驱动VAE、Attention、MoE等前沿技术发展的核心动力之一**。