# 第一部分：概率推断的基础

概率图模型（Probabilistic Graphical Models, PGM）作为一种强大的框架，在机器学习、人工智能和统计学领域占据着核心地位。它巧妙地将概率论与图论相结合，为表示和处理不确定性提供了一种直观且高效的语言。然而，构建模型仅仅是第一步，其真正的价值在于通过**推断（Inference）**来回答关于模型中变量的问题。本报告旨在对PGM中的推断任务进行详尽的阐述，从变量消元等基本精确推断方法入手，逐步深入到消息传递、因子图和近似推断等高级技术，并最终探讨这些经典思想在现代深度学习模型中的深刻影响和应用。
## 第一节：PGM中的核心推断任务

### 1.1 定义：概率、图与条件独立性
PGM的核心思想是**利用图结构来表示一个高维联合概率分布中变量之间的条件独立关系**。图中的节点（Nodes）代表随机变量，而边（Edges）则表示这些变量之间的概率依赖关系。图结构的一个关键优势在于，它使得**联合概率分布的紧凑表示**成为可能，因为图中缺失的边直接对应于特定的条件独立性假设。这极大地缓解了在处理成百上千个变量时，因状态空间呈指数级增长而带来的“维度灾难”问题。

PGM主要分为两大类：
- **贝叶斯网络 (Bayesian Networks, BNs):** 使用有向无环图（DAG）来表示变量间的因果或影响关系。在BN中，联合概率分布可以被分解为一系列局部条件概率的乘积，每个变量的条件概率只依赖于其父节点。其数学形式为：
    $$P(X1​,...,Xn​)=\prod_{i=1}^n​P(Xi​∣Parents(Xi​))$$
    这种分解不仅简化了计算，也提供了一种直观的方式来理解变量间的因果链条。
- **马尔可夫随机场 (Markov Random Fields, MRFs):** 使用无向图来表示变量间的非方向性相互关系。MRF的联合概率分布通常通过定义在图的团（Cliques）上的势函数（Potential Functions）来参数化。一个团是图中节点的一个子集，其中任意两个节点之间都存在边。其联合概率分布形式如下：
    $$p(x)=\frac{1}{Z}\prod_{C\in\mathcal{C}}\psi_C(\mathbf{x}_C)$$
    其中，$\mathcal{C}$是**图中所有（通常是最大）团的集合**，$\psi_C(\mathbf{x}_C)$是定义在团$C$上的非负势函数，而$Z$是归一化常数，被称为配分函数（Partition Function）。$Z$的作用是确保所有可能状态的概率之和为$1$，**其计算本身就是推断中的一个核心难题**。
### 1.2 查询模型：边缘、条件与证据
推断的本质是利用已建立的PGM，根据一组已观测到的变量（称为**证据 (Evidence)**）的值，来计算关于另一组未观测变量的概率信息。主要的推断查询类型包括：
- **边缘推断 (Marginal Inference):** 这是最基本的查询之一，旨在计算模型中某个变量子集的边缘概率分布。这需要通过对所有其他无关变量进行求和（离散情况）或积分（连续情况）来实现，即所谓的**边缘化（Marginalization）**。例如，计算单个变量
    $$P(X_i) = \sum_{X \setminus \{X_i\}} P(X)$$
    这个任务**对于评估特定事件发生的可能性至关重要**，例如，推断一个病人患有癌症的概率。
- **条件概率查询 (Conditional Probability Queries):** 这是**最常见也是最核心的推断任务**，其目标是计算在给定证据$E=e$的条件下，查询变量$Y$的后验概率分布$P(Y∣E=e)$。这类查询是==预测（如预测天气）和诊断（如根据症状诊断疾病）==等众多应用的基础。
- **配分函数$Z$的计算:** 对于MRF等无向模型，计算边缘或条件概率通常需要先得到归一化常数$Z$的值。然而，计算$Z$需要对所有可能的状态配置求和，在实践中极其困难。
### 1.3 寻求最可能解释：最大后验概率（MAP）推断
与计算概率分布不同，另一类重要的推断任务是寻找一个最可能的解释。
- **最大后验概率 (Maximum a Posteriori, MAP) 推断:** 此任务旨在找到一组未观测变量$Y$的最可能联合赋值，这个赋值在给定证据$E=e$的条件下具有最高的后验概率。其数学表达式为：
    $$Y^* = arg\underset{y}{\max} P(Y=y∣E=e)$$
    MAP推断在分类、解码和规划等领域有广泛应用，例如，在图像分割中找到最可能的像素标签分配。
- **MAP推断与边缘推断的关键区别:** 一个至关重要的区别是，**一组变量的最可能联合赋值（MAP解）不一定是由每个变量各自的最可能边缘赋值构成的**。例如，对于两个变量$Y_1$和$Y_2$，$\text{argmax}_{\mathbf{y_1, y_2}} P(y_1, y_2)$可能与$(\text{argmax}_{y_1} P(y_1), \text{argmax}_{y_2} P(y_2))$不同。这揭示了局部最优不等于全局最优的道理，也说明了为什么MAP推断是一个独立的、具有挑战性的问题。

从计算复杂度的角度看，MAP推断有时被认为比边缘推断“更简单”。这是因为MAP推断是一个优化问题（寻找最大值），而边缘推断是一个求和问题。在无向模型中，MAP推断可以忽略对配分函数$Z$的计算，因为$Z$不依赖于变量的赋值，不影响最大化的结果。尽管如此，MAP推断在一般情况下仍然是NP难问题。

所有这些推断任务的根本挑战在于，它们都需要以某种方式对一个可能跨越指数级巨大状态空间的联合分布进行全局性的求和或优化。正是这种固有的计算复杂性，催生了后续章节将要讨论的各种精确和近似推断算法。
## 第二节：精确推断之变量消元法
面对推断任务的计算挑战，研究者们首先开发了**精确推断算法。这些算法旨在给出准确的概率值**，但其适用性通常局限于特定结构的图模型。变量消元法（Variable Elimination, VE）是最基础也是最直观的精确推断算法。
### 2.1 暴力计算及其计算瓶颈
在介绍变量消元法之前，有必要先理解一种最朴素的计算方法：暴力枚举。对于一个边缘概率查询$P(X_i)$，暴力法会先构建出完整的联合概率分布表，然后对除$X_i$之外的所有变量的所有可能状态进行求和。假设模型中有$n$个二元变量，联合概率表的大小为$2^n$。为了计算一个变量的边缘概率，需要进行指数级别的加法和乘法运算。这种方法的计算成本随变量数量呈指数增长，因此对于绝大多数实际问题都是不可行的。
### 2.2 变量消元（VE）算法：利用分配律

变量消元法的核心思想是避免生成完整的联合概率分布，而是利用乘法对加法的分配律来智能地重排计算顺序，从而显著降低计算量。

考虑一个简单的链式贝叶斯网络$A→B→C→D→E$。其联合概率为$P(A,B,C,D,E)=P(A)P(B∣A)P(C∣B)P(D∣C)P(E∣D)$。若要计算边缘概率$P(E)$，暴力法需要对$A,B,C,D$的所有状态组合进行求和。**而VE算法则将求和操作尽可能地向内推**：

$$P(E)=\sum_D \sum_C \sum_B \sum_A P(A)P(B|A)P(C|B)P(D|C)P(E|D)=\sum_D P(E|D)\sum_C P(D|C)\sum_B P(C|B)\sum_A P(A)P(B|A)$$

计算从最内层的求和开始。首先，$\sum_A P(A)P(B|A)$会计算出一个只与变量$B$相关的中间项，我们称之为因子（factor），记作$m_A(B)$。这个过程“消元”了变量$A$。然后，这个新因子$m_A(B)$被用于下一层的计算：$\sum_B P(C|B)m_A(B)$，生成一个新的因子$m_B(C)$。这个过程持续进行，每次消去一个变量并生成一个更小的因子，直到最后只剩下查询变量$E$。
### 2.3 算法机制：因子乘积与边缘化
VE算法的执行过程可以被形式化为一系列在因子上的操作：
1. **因子乘积 (Factor Product):** 将两个或多个因子相乘，得到一个作用域（scope）为原因子作用域并集的新因子。例如，$\phi_3(A,B,C):=\phi_1(A,B)\times\phi_2(B,C)$。
2. **因子边缘化 (Factor Marginalization / Sum-out):** 对一个因子中的某个变量进行求和，得到一个作用域不再包含该变量的新因子。例如，$\tau(A,C)=\sum_B\phi(A,B,C)$。

==VE算法的完整流程如下：==
1. 给定一个查询和一组证据，首先将所有条件概率分布（CPDs）或势函数表示为初始因子。
 根据一个预先确定的**消元顺序**，依次处理要被消去的变量。
2. 对于当前要消元的变量$X_i$：
    a. 收集所有包含$X_i$的因子。
    b. 将这些因子相乘，得到一个大的中间因子。
    c. 对这个中间因子中的$X_i$进行边缘化（sum-out），生成一个不含$X_i$的新因子。
    d. 用这个新因子替换掉所有在步骤(a)中收集的旧因子。
3. 重复步骤3，直到所有非查询变量都被消去。
4. 将剩余的所有因子相乘并归一化，即可得到查询结果。
### 2.4 复杂性的核心：消元顺序与图的树宽

VE算法的**效率极度依赖于变量的消元顺序**。一个糟糕的顺序可能导致在算法执行过程中产生作用域非常大的中间因子，其大小是指数于作用域内变量数量的，从而使得计算变得不可行。

这个过程与图的结构有着深刻的联系。在消元变量$X_i$时，所有与$X_i$共同出现在某个因子中的变量（即$X_i$的邻居）现在都会出现在新生成的因子中。这相当于在原图的这些邻居节点之间添加了边，使它们形成一个团。这个过程被称为**图的三角化（triangulation）**，新加入的边被称为**填充边（fill-in edges）**。

VE算法的计算复杂度由整个消元过程中产生的最大因子的规模决定。这个最大因子的规模又由其作用域的大小（即最大团的规模）决定。一个图在特定消元顺序下的**诱导宽度（induced width）

**被定义为该过程中产生的最大团的大小减一。而一个图的**树宽（Treewidth）**则被定义为在所有可能的消元顺序中，能够达到的最小诱导宽度。

因此，VE算法的复杂度是关于图的树宽呈指数级，而关于变量数量呈线性。这意味着VE算法只对**低树宽**的图是有效的。树的树宽为$1$，因此VE在树状图上非常高效。然而，对于具有复杂环路结构和高连通性的图（如网格图），树宽可能很大，导致VE不可行。

不幸的是，寻找能最小化树宽的最优消元顺序本身就是一个NP难问题。在实践中，通常采用一些启发式策略来选择消元顺序，例如每次选择邻居最少的变量（min-neighbors）或导致填充边最少的变量（min-fill）。

这一分析揭示了一个深刻的结论：精确推断的内在计算难度并非由变量数量决定，而是由图模型的**拓扑连通性**决定，并由树宽这一图论属性精确量化。这为我们判断何时可以使用精确推断、何时必须转向近似方法提供了理论依据。
# 第二部分：统一框架与高级精确推断方法
变量消元法虽然直观，但其操作依赖于消元顺序，且每次查询都可能需要重新计算。为了克服这些局限性，研究者们发展了**更为通用和高效的精确推断框架，其中最核心的是基于消息传递**（Message Passing）的思想，而因子图（Factor Graphs）则为这一思想提供了最自然的表达方式。
## 第三节：因子图：一个统一的表示法

### 3.1 显式表示因子分解：从BN和MRF到因子图
因子图是一种二分图（Bipartite Graph），由两种类型的节点构成：**变量节点（Variable Nodes）和因子节点（Factor Nodes）** 。边只存在于变量节点和因子节点之间。每个因子节点代表联合概率分布分解式中的一个因子（或势函数），并与该因子所涉及的所有变量节点相连。

与贝叶斯网络或马尔可夫随机场不同，因子图将模型的**因子分解结构**进行了显式表达。在BN和MRF中，因子分解是隐含在父子关系或团结构中的，有时会产生歧义。例如，一个MRF中的单个势函数$\psi(X_1, X_2, X_3)$

可能对应于一个三节点团，但无法区分它是由一个三元交互作用产生的，还是由三个独立的二元交互作用$\psi(X_1, X_2)\psi(X_2, X_3)\psi(X_1, X_3)$产生的。因子图通过为每个因子$\psi$设立一个独立的因子节点，从而消除了这种歧义。
**模型转换**：
- **从贝叶斯网络到因子图:** 对于BN中的每个变量$X_i$，为其条件概率分布$P(X_i | \text{Parents}(X_i))$创建一个因子节点。该因子节点与变量节点$X_i$及其所有父节点相连。
- **从马尔可夫随机场到因子图:** 对于MRF联合概率表达式中的每一个势函数$\psi_C(\mathbf{x}_C)$，创建一个因子节点，并将其与团$C$中的所有变量节点相连。
这种统一的表示法极为重要，因为它允许我们设计一个通用的推断算法，该算法可以直接在因子图上操作，而无需关心原始模型是BN还是MRF。
### 3.2 因子图上的和积算法（置信度传播）
和积算法（Sum-Product Algorithm），更广为人知的名字是**置信度传播（Belief Propagation, BP）**，是一种在因子图上进行消息传递的算法，用于高效地计算所有变量的边缘概率。当因子图是**无环图（即树）**时，和积算法能够精确地计算出边缘概率，并且计算量远小于变量消元法。

该算法的核心思想是，图中的每个节点向其邻居节点发送“消息”。一条消息汇总了图的一部分信息，代表了发送节点对接收节点状态的“看法”或“置信度”。通过在整个图中迭代地传递和更新这些消息，最终每个变量节点可以整合来自所有方向的信息，从而计算出自己的边缘概率。

和积算法的普适性使其成为一个强大的统一框架。例如，用于隐马尔可夫模型（HMM）的**前向-后向算法（Forward-Backward Algorithm）**，可以被看作是在一个链式因子图上运行和积算法的一个特例。这揭示了许多看似不同的算法在本质上共享相同的计算核心：利用图的结构进行分布式计算。
### 3.3 消息更新规则的推导

和积算法的运行遵循两条简单、局部的消息更新规则，这些规则在变量节点和因子节点之间交替应用。
1. 从变量节点$x$到因子节点$f$的消息 ($\mu_{x \to f}(x)$):
    一个变量节点$x$发送给其邻接因子节点$f$的消息，是所有从其他邻接因子节点$h$发送给$x$的消息的乘积。这可以理解为，变量$x$将其从图中其他部分收到的所有信息整合起来，传递给$f$。
    $\mu_{x \to f}(x) = \prod_{h \in N(x) \setminus \{f\}} \mu_{h \to x}(x)$
    其中，$N(x)$表示变量节点$x$的邻居因子节点集合。如果$x$是一个叶子节点（只有一个邻居），它发送的消息为1（或均匀分布）。
2. 从因子节点$f$到变量节点$x$的消息 ($\mu_{f \to x}(x)$):
    一个因子节点$f$发送给其邻接变量节点$x$的消息，计算方式如下：首先，将因子$f$本身与所有从其他邻接变量节点$y$发送给$f$的消息相乘；然后，将结果中除了$x$之外的所有变量都进行边缘化（求和）。
    $\mu_{f \to x}(x) = \sum_{\mathbf{X}_f \setminus \{x\}} f(\mathbf{X}_f) \prod_{y \in N(f) \setminus \{x\}} \mu_{y \to f}(y)$
    其中，$\mathbf{X}_f$是因子$f$作用域内的所有变量集合，$N(f)$是因子$f$的邻居变量节点集合。$\sum_{\mathbf{X}_f \setminus \{x\}}$表示对$\mathbf{X}_f$中除$x$以外的所有变量求和。

在树状图中，消息可以从叶子节点开始，向内传递到一个任意指定的根节点，然后再从根节点向外传递回叶子节点。经过这两轮传递，图中每条边上双向的消息都已计算完毕。此时，任何一个变量节点$x$的（非归一化）边缘概率，或称为**置信度（Belief）**，就是所有指向它的消息的乘积：
$b(x) \propto \prod_{f \in N(x)} \mu_{f \to x}(x)$

最后，通过对$b(x)$进行归一化，即可得到精确的边缘概率$P(x)$。
### 3.4 联结树算法：通用图上的精确推断

对于含有环路（loopy）的图，直接应用BP算法（即**Loopy BP**）不能保证收敛，即使收敛，结果也通常是近似的而非精确的。为了在任意拓扑结构的图上实现精确推断，研究者们提出了

**联结树算法（Junction Tree Algorithm）**，也称团树（Clique Tree）算法。

该算法的核心思想是将一个任意的图转化为一个等价的、无环的**树状结构**，即联结树。这个树的节点不再是单个变量，而是原始图中的**团（cliques）**。

构建联结树的步骤通常包括：

1. **道德化 (Moralization):** 如果原始图是贝叶斯网络（有向图），首先将其转化为无向图。具体操作是：连接每个节点的父节点，然后将所有边的方向去掉。

2. **三角化 (Triangulation):** 在道德化图的基础上，通过添加额外的“填充边”来消除图中所有长度大于3的环，使其成为一个**弦图（Chordal Graph）**。

3. **构建团树:** 识别出弦图中的所有最大团，将这些团作为联结树的节点。然后，在团之间添加边，使得最终形成的图是一个树，并满足**运行交集特性（running intersection property）**：如果一个变量同时出现在两个团节点中，那么它必须也出现在连接这两个团节点的路径上的所有团节点中。

一旦联结树构建完成，就可以在其上运行一个与和积算法非常相似的消息传递算法。消息在相邻的团节点之间传递，最终计算出每个团的精确边缘概率。由于每个变量都至少存在于一个团中，因此所有单变量的边缘概率也能被精确计算出来。

联结树算法的计算复杂度仍然由其结构决定，具体来说，是与原始图的**树宽**成指数关系。因为联结树中最大的团节点的大小就对应了该图的树宽。因此，联结树算法虽然通用，但它并没有改变精确推断的根本性难题：对于高树宽的图，它依然是不可行的。

## 第四节：MAP推断与能量最小化

在许多应用中，我们的目标并非计算概率，而是找到最可能的变量配置，即执行MAP推断。这一任务同样可以利用消息传递框架解决，并与优化领域的能量最小化问题有着深刻的联系。

### 4.1 最大积算法：为最可能状态设计的消息传递

为了解决MAP推断问题，我们可以对和积算法进行一个简单的修改：将算法中的**求和（$\sum$）**操作替换为**最大化（max）**操作。这样得到的算法被称为**最大积算法（Max-Product Algorithm）**。如果在对数域中操作，加法替换了乘法，该算法则被称为

**最大和算法（Max-Sum Algorithm）**。

最大积算法的消息更新规则与和积算法极为相似：

- **从变量到因子消息:** $\mu_{x \to f}(x) = \prod_{h \in N(x) \setminus \{f\}} \mu_{h \to x}(x)$ （与和积算法相同）

- **从因子到变量消息:** $\mu_{f \to x}(x) = \max_{\mathbf{X}_f \setminus \{x\}} f(\mathbf{X}_f) \prod_{y \in N(f) \setminus \{x\}} \mu_{y \to f}(y)$

与和积算法一样，最大积算法在树状图上是精确的。著名的**维特比算法（Viterbi Algorithm）**，用于在隐马尔可夫模型（HMM）等链式结构中寻找最可能的状态序列，正是最大积算法在链式图上的一个经典实例。

### 4.2 解码MAP状态：回溯过程

当最大积算法在树状图上运行收敛后，每个变量节点$i$的“置信度”$b_i(x_i) = \max_{\mathbf{X} \setminus \{x_i\}} P(X)$，代表了包含$x_i$的最优配置的概率值，这被称为**最大边缘概率（max-marginal）**。需要注意的是，简单地对每个变量独立地取其最大边缘概率对应的状态，并不能保证得到全局最优的联合MAP配置。

为了解码出一致的联合MAP状态，必须采用回溯（backtracking）过程。在执行最大积算法的正向消息传递时，除了计算消息的值，还必须为每个消息$\mu_{f \to x}(x)$记录下是哪个（些）上游变量的配置使得该消息取得了最大值。这些记录被称为

回溯指针（backpointers）。

算法收敛后，解码过程如下：

1. 选择一个根节点，计算其最优状态$x_{\text{root}}^* = \arg\max_{x_{\text{root}}} b(x_{\text{root}})$。

2. 从根节点开始，利用存储的回溯指针，反向推导出其邻居节点的最优状态。
    
3. 递归地进行这个过程，直到图中所有变量都被赋予了最优状态，从而恢复出完整的MAP配置。
    

### 4.3 MAP作为能量最小化：通往优化的桥梁

MAP推断问题可以被等价地表述为一个**能量最小化（Energy Minimization）**问题，这一视角源自统计物理学。一个配置$\mathbf{x}$的概率可以表示为$p(\mathbf{x}) \propto \exp(-E(\mathbf{x}))$，其中$E(\mathbf{x})$是该配置的能量。最大化概率$p(\mathbf{x})$等价于最小化能量$E(\mathbf{x})$。

对于成对马尔可夫随机场（pairwise MRF），能量函数通常写成一元势和成对势之和的形式：

$$
E(\mathbf{x}) = \sum_{i \in V} \theta_i(x_i) + \sum_{(i,j) \in E} \theta_{ij}(x_i, x_j)
$$

其中$\theta_i(x_i)$是与数据相关的惩罚项（data term），$\theta_{ij}(x_i, x_j)$是鼓励邻近变量取相似值的平滑项（smoothness term）。这种表述方式将MAP推断问题与组合优化领域紧密地联系起来，使得我们可以应用该领域的强大工具来求解。

### 4.4 案例研究：图割用于二元成对MRF的MAP推断

对于特定类型的能量函数，MAP推断问题可以通过**图割（Graph Cuts）**算法精确且高效地求解。这一方法极为强大，因为它甚至适用于具有高树宽的稠密图（如图像网格），而这类图对于变量消元或联结树算法是不可行的。

该方法的核心在于构建一个特殊的**s-t图**，图中包含一个源点$s$和一个汇点$t$。图中的一个**s-t割（s-t cut）**是将所有节点划分为两个不相交的集合$S$和$T$（其中$s \in S, t \in T$）的分割。割的**容量（capacity）**是从$S$集合跨越到$T$集合的所有边的权重之和。

**图的构建过程（针对二元变量$x_i \in \{0,1\}$）**：

1. **节点:** MRF中的每个变量节点$i$都对应s-t图中的一个普通节点。此外，图中还包含源点$s$和汇点$t$。

2. **t-links:** 一元势$\theta_i(x_i)$被映射为连接变量节点$i$与两个终端$s, t$的边（称为t−links）。通常，将$s$与标签‘1’关联，将$t$与标签‘0’关联。边$(s, i)$的权重与将$i$标记为‘0’的惩罚（即$\theta_i(0)$）相关，而边$(i, t)$的权重与将$i$标记为‘1’的惩罚（即$\theta_i(1)$）相关。

3. **n-links:** 成对势$\theta_{ij}(x_i, x_j)$被映射为连接变量节点$i$和$j$的边（称为n−links）。其权重与$x_i$和$x_j$取不同标签时的不一致性惩罚相关。

这种图的构建方式非常精妙，它保证了任意一个s-t割的容量都精确地等于MRF中对应二元标签分配的能量。因此，找到最小容量的s-t割就等价于找到了最小能量的MAP解。

**最大流-最小割定理（Max-Flow Min-Cut Theorem）**是这一方法得以高效求解的理论基石。该定理指出，在一个流网络中，从源点$s$到汇点$t$的最大流量等于该网络最小s-t割的容量。由于存在多项式时间的算法（如Boykov-Kolmogorov算法）来计算最大流，我们也就能够高效地找到最小割，从而求解MAP问题。

**子模性（Submodularity）**是图割方法能够给出精确解的关键条件。对于二元成对MRF，能量函数是子模的，当且仅当其成对势满足以下不等式：

$$
\theta_{ij}(0,0) + \theta_{ij}(1,1) \leq \theta_{ij}(0,1) + \theta_{ij}(1,0)
$$

这个条件直观上意味着“一致性”（两个变量取相同标签）的能量惩罚之和，小于“不一致性”（两个变量取不同标签）的能量惩罚之和。从图构建的角度看，子模性保证了所有n-links的权重都是非负的，这是标准最大流算法的必要前提。

这一发现揭示了推断复杂性的另一个维度：除了图的拓扑结构（树宽），势函数的**数学性质**（如子模性）同样决定了问题是否可解。对于满足子模性的二元MAP问题，我们可以绕过树宽的限制，获得高效的精确解。

# 第三部分：近似推断：应对棘手问题

当精确推断因图结构复杂（树宽过高）或势函数不满足特定性质（如非子模）而变得不可行时，我们必须求助于近似推断算法。这些算法放弃了对精确解的保证，以换取在计算时间和资源上的可行性。主要有两大类方法：基于优化的变分推断和基于采样的蒙特卡洛方法。

## 第五节：变分推断（Variational Inference）

### 5.1 核心原理：用易解问题近似难解问题

变分推断（VI）是一种确定性的近似推断方法，其核心思想是将推断问题转化为一个优化问题。我们知道，计算后验概率$p(Z|X)$之所以困难，主要是因为其分母上的证据（evidence）

$$
p(X)=\int p(X,Z)dZ
$$

难以计算。

VI的策略是，不再直接计算这个复杂的真实后验$p(Z|X)$，而是引入一个来自某个相对简单的、参数化的分布族$\mathcal{Q}$中的分布$q(Z)$，来近似$p(Z|X)$。然后，通过优化$q(Z)$的参数，使其尽可能地“接近”真实的后验分布。这种“接近”程度通常用**Kullback−Leibler(KL)散度**来度量，目标是最小化$D_{KL}(q(Z) || p(Z|X))$。

的参数，使其尽可能地“接近”真实的后验分布。这种“接近”程度通常用∗∗Kullback−Leibler(KL)散度∗∗来度量，目标是最小化$D_{KL}(q(Z) || p(Z|X))$。

### 5.2 近似后验：平均场假设

选择一个合适的变分族$\mathcal{Q}$至关重要。一个非常流行且简单的选择是∗∗平均场（Mean−Field）∗∗变分族。该假设的核心是，近似分布$q(Z)$可以被分解为一系列相互独立的因子的乘积：

$$
q(Z)=\prod_{j=1}^{m} q_j(z_j)
$$

这里，$Z = {z_1,..., z_m}$是隐变量的集合。这个假设极大地简化了问题，因为它将一个原本变量间相互耦合的复杂后验分布，拆解成了一组独立的、易于处理的单变量分布。

在平均场假设下，可以通过坐标上升法（Coordinate Ascent）进行优化。可以证明，对于每个因子$q_j(z_j)$，其最优解$q_j^*(z_j)$的形式为：

$$\log q_j^*(z_j) \propto \mathbb{E}_{i \neq j} [\log p(X, Z)]$$

即，最优的$q_j(z_j)$正比于联合概率的对数在所有其他因子$q_{i \neq j}$的分布下的期望。这个结果引出了一种迭代更新的算法，直至收敛。

### 5.3 证据下界（ELBO）：推导及其作用

直接最小化KL散度$D_{KL}(q(Z) || p(Z|X))$仍然是不可行的，因为其定义中包含了我们试图避免计算的证据项$p(X)$。VI的精妙之处在于，它找到了一个等价的优化目标。通过对数证据$\log p(X)$进行分解，我们可以得到以下恒等式：

$$\log p(X) = \mathcal{L}(q) + D_{KL}(q(Z) || p(Z|X))

$$其中，$\mathcal{L}(q)$被称为**证据下界（Evidence Lower Bound, ELBO）**，其定义为：$$

\mathcal{L}(q) = \mathbb{E}_{q(Z)}[\log p(X,Z)] - \mathbb{E}_{q(Z)}[\log q(Z)]$$

这个分解是VI的理论基石。由于KL散度总是非负的（$D_{KL} \ge 0$），我们立刻得到$\log p(X) \ge \mathcal{L}(q)$，这正是ELBO名称的由来。更重要的是，对于我们的优化问题，$\log p(X)$是一个关于$q$的常数。因此，最大化ELBO就等价于最小化KL散度。

ELBO的表达式中只包含联合概率$p(X,Z)$（通常是可计算的，因为它只是模型因子的乘积）和近似分布$q(Z)$，完全避开了对证据$p(X)$的计算。这样，VI就成功地将一个棘手的积分问题转化为了一个（可能非凸，但通常可处理的）优化问题。

### 5.4 Loopy BP作为变分推断：Bethe自由能

当置信度传播（BP）算法被应用于有环图时（称为**Loopy BP**），它失去了精确性的保证，但经验表明它常常能给出非常好的近似结果。这种现象一度缺乏理论解释，直到人们发现LBP与变分推断之间存在深刻的联系。

LBP的定点（fixed points）被证明与一个特定的变分目标函数的稳定点相对应，这个目标函数被称为**Bethe自由能（Bethe Free Energy）**。

Bethe自由能是一种比平均场更复杂的变分近似。它不要求所有变量完全独立，而是通过一个“树状”的熵近似来构造目标函数。优化的对象是一组“伪边缘概率”（pseudo-marginals），这些伪边缘概率只需要满足局部一致性（例如，任意边$(i,j)$的边缘概率$q_{ij}(x_i, x_j)$在对$x_j$求和后，应与单节点边缘概率$q_i(x_i)$一致）。

LBP算法可以被看作是试图通过消息传递来求解Bethe自由能最小化问题的一种迭代算法。这一发现为LBP的有效性提供了理论支撑：它不再是一个纯粹的启发式算法，而是有明确优化目标的、有原则的近似方法。

## 第六节：蒙特卡洛方法

与确定性的变分推断不同，蒙特卡洛方法是一类基于随机采样的近似推断技术。其核心思想是通过从目标分布中抽取大量样本，然后利用这些样本来估计期望、边缘概率等感兴趣的量。

### 6.1 通过模拟进行近似：MCMC范式

马尔可夫链蒙特卡洛（Markov Chain Monte Carlo, MCMC）是一族核心的采样算法。当直接从一个复杂的目标后验分布$P(Y|E=e)$中采样非常困难时，MCMC方法会构建一个特殊的马尔可夫链，这个链的平稳分布（stationary distribution）恰好就是我们的目标分布$P(Y|E=e)$。

算法从一个随机状态开始，然后根据马尔可夫链的转移规则不断生成新的样本。在经过一段“燃烧期（burn-in）”以确保马尔可夫链达到平稳分布后，后续生成的样本就可以被看作是来自目标分布的（相关的）样本。通过收集足够多的样本，我们可以近似计算各种统计量。

### 6.2 吉布斯采样：从条件分布中迭代重采样

**吉布斯采样（Gibbs Sampling）**是一种非常流行且特别适用于PGM的MCMC算法。它的基本思想非常简单：不直接对整个高维联合分布进行采样，而是轮流对每个变量进行一维采样。

吉布斯采样的算法流程如下：

1. 对所有变量$\mathbf{X} = {X_1,..., X_n}$进行随机初始化，得到初始状态$\mathbf{x}^{(0)}$。

2. 进行T次迭代，对于第t次迭代：
    
    a. 从$P(X_1 | x_2^{(t-1)}, x_3^{(t-1)},..., x_n^{(t-1)})$中采样得到$x_1^{(t)}$。
    
    b. 从$P(X_2 | x_1^{(t)}, x_3^{(t-1)},..., x_n^{(t-1)})$中采样得到$x_2^{(t)}$。
    
    c. ...
    
    d. 从$P(X_n | x_1^{(t)}, x_2^{(t)},..., x_{n-1}^{(t)})$中采样得到$x_n^{(t)}$。
    
3. 由${\mathbf{x}^{(1)}, \mathbf{x}^{(2)},..., \mathbf{x}^{(T)}}$构成的样本集合可用于近似目标分布。
    

在每一步中，采样一个变量时，都以所有其他变量的**最新**值为条件。

### 6.3 马尔可夫毯在高效采样中的作用

吉布斯采样中的条件概率$P(X_i | \mathbf{X}_{-i})$（其中$\mathbf{X}_{-i}$表示除$X_i$外的所有变量）看似需要以模型中所有其他变量为条件，计算量巨大。然而，PGM的图结构再次提供了关键的简化。根据图的条件独立性，一个变量$X_i$在给定其**马尔可夫毯（Markov Blanket, MB）**的条件下，与其马尔可夫毯之外的所有其他变量都是条件独立的。

因此，吉布斯采样中的条件采样步骤可以被极大地简化为：

$$P(X_i | \mathbf{X}_{-i}) = P(X_i | \text{MB}(X_i))$$

马尔可夫毯的定义取决于图的类型：

- 在**贝叶斯网络**中，节点$X_i$的马尔可夫毯包括其**父节点、子节点、以及其子女的其他父节点（co-parents）**。

- 在**马尔可夫随机场**中，节点$X_i$的马尔可夫毯就是其**直接邻居节点**。

由于马尔可夫毯通常只包含模型中的一小部分局部变量，这意味着计算$P(X_i | \text{MB}(X_i))$只需要考虑与$X_i$直接相关的因子，计算量大大降低。这使得吉布斯采样成为一种在PGM中进行近似推断的、计算上局部且高效的强大工具。

|范式|算法族|具体算法|核心任务|保证|复杂度驱动因素|主要用例|
|---|---|---|---|---|---|---|
|**精确推断**|变量消元|变量消元法|边缘/条件/MAP|精确|图的树宽|低树宽图模型|
||消息传递|联结树算法|边缘/条件/MAP|精确|图的树宽|需要精确解的通用图|
|||和积算法 (BP)|边缘/条件|在树上精确|图结构（环路）|树状图，LBP的基础|
|||最大积算法|MAP|在树上精确|图结构（环路）|树状图，维特比算法的基础|
|**近似推断**|变分推断|平均场VI|边缘/条件|确定性界|变分族的选择|模型简单，需要快速确定性近似|
|||Loopy BP|边缘/条件/MAP|无保证（经验有效）|图的环路结构|通用有环图，常用于计算机视觉等|
||MCMC采样|吉布斯采样|边缘/条件|渐进无偏|样本数量/混合时间|复杂高维模型，难以确定变分族|

# 第四部分：PGM推断思想在现代人工智能中的传承

概率图模型中发展出的推断原理，特别是动态规划、变分优化和消息传递等核心思想，并未随着深度学习的兴起而过时。相反，它们被吸收、改造并深度集成到现代神经网络架构中，成为解决复杂结构化预测和生成任务的理论基石。

| PGM原理/算法               | 现代深度学习模型               | 角色与联系                                                                                                       |
| ---------------------- | ---------------------- | ----------------------------------------------------------------------------------------------------------- |
| **链式图上的最大积推断 (维特比算法)** | **BiLSTM-CRF**         | CRF层是一个线性链PGM。维特比算法作为解码步骤，用于寻找最可能的标签序列，从而在神经网络输出之上强制施加了标签间的结构化约束。                                           |
| **变分推断 & 证据下界 (ELBO)** | **变分自编码器 (VAE), 扩散模型** | ELBO是VAE的核心损失函数。VAE的编码器学习一个_摊销式_的推断网络来近似棘手的后验分布，整个系统通过最大化数据对数似然的下界进行训练。                                     |
| **置信度传播 (消息传递)**       | **图神经网络 (GNN)**        | GNN的核心操作是一种广义的、_可学习的_消息传递。节点嵌入通过聚合来自邻居的消息（特征向量）并使用可学习的函数（神经网络）进行更新，这与BP使用固定函数（求和/最大化）更新置信度（势函数）的过程形成了深刻的类比。 |

## 第七节：序列模型中的推断：BiLSTM-CRF

### 7.1 CRF层作为线性链PGM

在诸如命名实体识别（NER）等序列标注任务中，如果仅使用一个简单的Softmax层对每个时间步（词元）独立进行分类，会忽略标签之间存在的强依赖关系。例如，在BIO标注体系中，“I-PER”（人名中间部分）标签几乎总是跟在“B-PER”（人名开始部分）或另一个“I-PER”之后，而不可能出现在“O”（非实体）标签之后。

为了对这种序列结构进行建模，研究者们在双向长短期记忆网络（BiLSTM）的输出层之上增加了一个条件随机场（Conditional Random Field, CRF）层。这个CRF层，具体来说是一个

线性链CRF，其本质就是一个简单的无向概率图模型（一条链），其中每个节点代表序列中一个位置的标签。

在这个混合架构中，BiLSTM层负责从输入序列中提取丰富的上下文特征，其输出可以被看作是为CRF模型提供了**一元势（unary potentials）**，即在给定输入词的条件下，每个位置被赋予某个标签的可能性（也称为发射分数）。而CRF层自身则学习一个**成对势（pairwise potentials）**，通常表现为一个**转移矩阵**，该矩阵编码了从一个标签转移到下一个标签的得分。整个模型的得分函数综合了发射分数和转移分数。

### 7.2 维特比算法作为最大积推断进行解码

在模型训练完成后，进行推断（或称解码）的目标是：对于一个给定的输入序列，找到具有最高联合概率的标签序列。这正是在线性链CRF上执行的MAP推断任务。

**维特比算法**被用来精确且高效地解决这个MAP问题 102。维特比算法是一种动态规划方法，它通过迭代计算在每个时间步到达每个可能标签的最大累积分数，并存储导致该最大分数的路径（回溯指针），最终找到全局最优的标签序列。

从PGM推断的角度看，维特比算法与前述的最大积算法在形式上是等价的。它正是在一个链式结构的图上执行最大积消息传递的过程 53。这体现了PGM推断算法在现代深度学习模型中直接、非隐喻性的应用：一个经典的精确推断算法被用作神经网络的一个模块，以引入对输出结构的先验知识和约束。

## 第八节：生成模型与摊销式推断

### 8.1 变分自编码器（VAE）的损失函数即ELBO

变分自编码器（VAE）是一类深度生成模型，它通过引入一个低维的隐变量$z$来学习数据的复杂分布$P(x)$。模型由两部分组成：一个先验分布

$p(z)$（通常是标准正态分布）和一个解码器网络$p_\theta(x|z)$，后者从隐变量$z$生成数据$x$。

通过最大似然来训练VAE需要计算后验分布$p_\theta(z|x)$，但这通常是**不可解的（intractable）**，因为它需要计算证据$p_\theta(x)=\int p_\theta(x|z)p(z)dz$，这个积分没有解析解。

VAE的解决方案与变分推断如出一辙。它引入了一个额外的编码器网络$q_\phi(z|x)$，作为真实后验$p_\theta(z|x)$的**近似**。整个模型通过最大化**证据下界（ELBO）**进行端到端训练。VAE的损失函数正是负ELBO：

$$-\mathcal{L}(\theta, \phi) = \mathbb{E}_{q_\phi(z|x)}[-\log p_\theta(x|z)] + D_{KL}(q_\phi(z|x) | p(z))$$

损失函数的第一项是重构损失，它促使解码器能够从隐变量中恢复原始数据。第二项是KL散度，它作为一个正则化项，迫使编码器产生的近似后验分布$q_\phi(z|x)$接近于先验分布$p(z)$。

### 8.2 编码器作为学习到的摊销式推断网络

在经典的变分推断中，为了给**每一个**数据点找到其对应的最优近似后验$q(z)$，都需要运行一次迭代优化过程。这种方式效率低下。

VAE的编码器网络$q_\phi(z|x)$则实现了一种**摊销式推断（Amortized Inference）**。它是一个由参数$\phi$决定的单一函数（神经网络），能够直接将任意观测数据$x$映射到近似后验分布$q(z|x)$的参数（例如高斯分布的均值和方差）。这意味着，一旦模型训练完成，对新数据进行推断就不再需要迭代优化，而只需一次前向传播即可，极大地提高了推断效率。

### 8.3 扩散模型作为层级隐变量模型

近年来大放异彩的**扩散模型（Diffusion Models）**，也可以被理解为一种特殊的层级隐变量模型。它们构建了一个马尔可夫链，其中隐变量的维度与原始数据相同，通过逐步向数据中添加噪声（前向过程）并学习如何逆转这个过程（反向过程）来进行生成。

值得注意的是，扩散模型的训练目标同样可以被推导为一个作用于数据对数似然的变分下界（ELBO）。这再次表明，变分推断的基本原理已经成为现代高级生成模型设计的核心理论支柱之一。

## 第九节：图神经网络作为学习到的消息传递

### 9.1 GNN消息传递与置信度传播的内在联系

图神经网络（GNN）的核心运作机制是一个**消息传递（Message Passing）**框架。 在GNN的每一层中，每个节点都会从其邻居节点收集“消息”，然后结合自身信息来更新自己的表示（或称嵌入，embedding）。

这个过程在概念和形式上都与PGM中的置信度传播（BP）算法高度相似。

- 在**BP算法**中，消息是关于变量状态的概率分布或势函数，而聚合与更新规则是固定的数学运算（如和积、最大积）。
    
- 在**GNN**中，消息是高维的特征向量（嵌入），而聚合与更新函数是**可学习的**神经网络模块（如多层感知机MLP、门控循环单元GRU等）。
    

### 9.2 学习消息与更新函数

可以将GNN看作是在“学习”一个消息传递推断算法。GNN不是依赖于BP中预先定义的、基于概率论的固定规则，而是通过反向传播和梯度下降，为特定的下游任务（如节点分类、链接预测）学习最优的消息构造、聚合与更新函数。

这一深刻的联系催生了许多前沿研究。例如，一些工作通过“展开”LBP的迭代过程，并将固定的消息计算函数替换为神经网络，构建了所谓的**因子图神经网络（Factor Graph Neural Network, FGNN）**。这种混合模型允许端到端地学习一个类推断过程，并且在实验中，它能够在有环图上取得比传统LBP更好的性能，因为它能通过学习来适应和补偿环路带来的信息重复计算问题。

# 第五部分：综合与结论

## 第十节：结论

本报告系统地梳理了概率图模型中的推断任务与方法，描绘了一条从基础理论到前沿应用的清晰路径。我们的探索始于推断问题的核心——在由图结构定义的、紧凑表示的高维概率空间中，如何高效地回答关于变量的查询。这一需求引出了两大类基本任务：计算边缘概率（边缘推断）和寻找最可能配置（MAP推断）。

我们首先分析了**精确推断**。变量消元法（VE）作为最直观的精确算法，其核心在于利用分配律智能地重排计算顺序。然而，对VE复杂性的深入分析揭示了一个根本性的结论：精确推断的固有难度并非由变量数量决定，而是由图的拓扑连通性所决定，并由**树宽**这一图论概念精确量化。这一发现界定了精确推断的适用边界，即仅限于低树宽的图模型。为了处理更通用的图，联结树算法通过将任意图转化为等价的团树，将消息传递的思想推广为一种通用的精确推断框架，但其复杂度依然受限于树宽。

当精确推断不可行时，**近似推断**成为必然选择。本报告探讨了两种主流范式。**变分推断（VI）**将棘手的积分问题转化为优化问题，通过最大化证据下界（ELBO）来寻找一个简单分布以逼近真实的复杂后验。这一思想不仅解释了经典Loopy BP算法的有效性（其定点对应于Bethe自由能的稳定点），更成为了现代生成模型（如VAE和扩散模型）的理论基石。另一大范式是**马尔可夫链蒙特卡洛（MCMC）**方法，如吉布斯采样，它通过构建马尔可夫链从目标分布中进行随机抽样。PGM的**马尔可夫毯**结构使得采样过程从全局依赖简化为高效的局部计算，展示了图结构在简化随机模拟中的威力。

最后，本报告的核心论点在于，PGM推断的这些经典思想非但没有被深度学习所取代，反而以一种更广义、更强大的形式被继承和发扬。

- **动态规划**的思想，如维特比算法（最大积推断），在BiLSTM-CRF等模型中作为结构化输出层，用于强制实施序列标签的语法约束。
    
- **变分优化**的框架，尤其是ELBO，直接构成了VAE等深度生成模型的损失函数，解决了其中棘手的后验推断问题。
    
- **消息传递**的范式，在图神经网络（GNN）中得到了新生。GNN可以被视为一种**可学习的置信度传播**，它将BP中固定的消息更新规则替换为灵活的、可通过数据驱动方式优化的神经网络，从而将推断过程本身融入了端到端的学习框架中。
    

总而言之，从变量消元到图神经网络，推断技术的发展历程反映了人工智能领域在处理不确定性和结构化数据方面持续的探索与演进。PGM提供的结构化概率建模语言及其推断算法，不仅是解决过去许多问题的关键，更已成为驱动当前深度学习模型向更复杂、更鲁棒、更可解释方向发展的核心理论动力。未来，概率推理与深度学习的进一步融合，必将继续催生出更加强大的智能系统。