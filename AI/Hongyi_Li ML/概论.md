#ai
==AI = LAMBDA==
* L: loss function
* A: **Algorithm**
* M: **Model**
* BD: Big **Data**
* A: Application
## 基本的学习任务
#人工智能种类 
**判别式**
1. classification
2. regression
3. decision
**生成式**
4. structure learning
## Loss
#error_surface
* 输出是一个“值”：**MA**bsolute**E**、**MS**quare**E**
* 输出是一个“分布”：Cross-Entropy

loss随参数变化而变化，可以绘制出一个多维等高线图，称为==Error Surface==
![[Pasted image 20250605203200.png]]
## Algorithm
> 算法的目的是**最小化loss**

最常用方法：Gradient Descent

局部最优、全局最优
local minimal：不是真正的问题？

## Model
#Sigmoid
![[Pasted image 20250605204726.png]]
![[Pasted image 20250605204946.png]]
特征、矩阵、向量、未知参数
![[Pasted image 20250605205546.png]]
#epoch #batch #iteration
![[Pasted image 20250605210203.png]]
#relu
![[Pasted image 20250605210422.png]]

# Guidance
![[Pasted image 20250606190029.png]]
注意：mismatch和overfiting不同，**前者表明测试数据和训练数据不同分布**，无法通过增加训练数据（原来的分布）解决。
## overfiting
#overfiting
![[Pasted image 20250606190553.png]]
偏差-方差 trade-off
模型的拟合能力和复杂度一般是正相关的
![[Pasted image 20250606190719.png]]
==注意：绝对不能面向测试集调参，泄露数据。应该使用交叉验证==
![[Pasted image 20250606191440.png]]
#交叉验证 选出最优模型，再**使用全部的训练集**得到新的模型