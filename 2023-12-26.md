# 跑通AID-Bio for Meta-Learning
* [x] 解决`autograd`为`None`问题
    `RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.`
* [x] 解决越训练越差，`Loss`值不下降问题
      怀疑：`warm hot`:使用的是`head_global`,每次`fast()`使用的都是上一次更新后的`head`。
      `Solved`，2023年12月26日22:15:22。在`iteration`外层，保存初始化的`head`参数并作为`fast()`的`input`。而在`fast()`内部，重置`head_global`参数为其初始值。
```
```
``` python
head_global = torch.nn.Linear(head_dim, ways).to(device)
# Store initial state
initial_head_global_state = [p.clone().detach() for p in head_global.parameters()]
```
      
``` python
def fast(...):
    # Reset head_global to its initial state
    with torch.no_grad():
        for p, initial_p in zip(head_global.parameters(), initial_head_global_state):
            p.copy_(initial_p)

    # Rest of the function implementation

```