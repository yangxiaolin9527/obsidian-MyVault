# 无人机智能感知、协同与优化
> 聚焦于无人机（Unmanned Aerial Vehicle, UAV）系统，特别是无人机集群（UAV Swarm）的智能化和自主化

## 宏观理解
目标是突破人类遥控的限制，使无人机（或集群）能够在未知、动态甚至充满对抗的复杂环境中，自主完成任务。这要求无人机不仅仅是一个“会飞的传感器”，而是一个具备**感知、认知、决策和行动能力**的完整智能系统。其研究范畴涵盖了从单个智能体的底层控制到多智能体系统的高层战略。
## 核心模块
### 智能感知与认知
> 核心问题：无人机如何利用自身有限的传感器，构建对三维世界全面而准确的理解？

1. 多源异构数据融合：无人机通常搭载多种传感器，如可见光相机（提供纹理和颜色）、红外相机（感知热量，用于夜间或伪装目标识别）、激光雷达 (LiDAR)（直接获取精确的三维点云）、雷达 (Radar)（测速、穿透性好）以及惯性测量单元 (IMU)（提供自身姿态）。这些数据时间上未对齐、空间上存在偏差，物理含义不同，如何进行有效的融合与处理是一个关键问题，涉及到**时空对齐、噪声建模与置信度评估**。
2. 实时语义理解：在融合数据的基础上，无人机需要“看懂”世界。这包括目标检测（找到汽车、人）、语义分割（区分出天空、建筑、道路）、三维场景重建等。在资源受限的机载计算平台上实现这些任务的实时性是巨大的工程挑战。
### 协同控制与决策
> 如何组织和协调大量的无人机，使它们作为一个整体，高效、鲁棒地完成单个无人机无法完成的复杂任务（如协同搜索、区域覆盖、饱和攻击）？

1. 分布式协同控制：分布式协同控制：对于大规模集群，中心化的控制节点是性能瓶颈和单点故障源。因此，研究的重点是分布式算法，每个无人机仅依赖于与邻近无人机的通信，通过局部交互涌现出全局的、宏观的有序行为。这涉及到一致性理论 (Consensus Theory)、编队控制 (Formation Control)等。
2. 动态任务分配与冲突消解：在任务执行过程中，情况是动态变化的（如发现新目标、某架无人机失效）。集群需要能够实时地重新分配任务，以达到全局最优。这通常被建模为组合优化问题，常用方法包括拍卖算法 (Auction-based Algorithms)、市场机制等。同时，无人机在执行任务时需要进行航迹规划 (Trajectory Planning) 和冲突消解，避免碰撞。

### 系统优化与学习
> 核心问题：如何让无人机的能力，特别是其“智能”，能够通过学习不断进化和优化？

1. Sim2Real：在真实世界中训练无人机（尤其是集群和涉及破坏性的任务）成本高、风险大、效率低。因此，在仿真环境中进行大规模训练是必经之路。但仿真与现实之间存在现实差距 (Reality Gap)（如空气动力学模型不准、传感器噪声模型不一致）。Sim2Real 技术就是为了弥合这一差距，常用方法包括域随机化 (Domain Randomization)（在仿真中引入大量随机变化，使模型对不确定性更鲁棒）和域适应 (Domain Adaptation)。
2. 在线学习/持续学习：无人机部署后，会遇到仿真中从未见过的新场景。它需要具备在线学习的能力，在执行任务的同时不断微调自己的模型以适应新环境，同时不遗忘旧知识（避免灾难性遗忘）。

## 人工智能交叉
### AI for Perception
1. 多模态融合模型：基于 Transformer 的架构（特别是其自注意力机制）在融合不同模态的数据上显示出巨大潜力。通过跨模态注意力 (Cross-Modal Attention)，模型可以学习到例如“图像中这片区域”和“点云中这个三维结构”指的是同一个物体。
2. 自监督学习：在野外环境中，获取大量标注数据是困难的。可以利用自监督学习，让无人机从无标注的视频流或多种传感器数据间的关联性中自己学习有用的特征表示。
### AI for Decision and Collaboration
1. 多智能体强化学习 (Multi-Agent Reinforcement Learning, MARL)：MARL 是解决无人机集群协同问题的理想范式。
    - 每个无人机都是一个 Agent。它们共同与环境交互，并根据一个共享的（或各自的）奖励信号来学习自己的策略。
    - 核心挑战在于：1) 环境的非平稳性（一个Agent的策略在变，对其他Agent来说环境就在变）；2) 信用分配（团队成功了，如何奖励每个成员？）。
    - 主流的 MARL 框架如 CTDE (Centralized Training with Decentralized Execution) 非常适合无人机场景：在训练时（通常在仿真器里），可以利用全局信息指导所有无人机学习；在执行时，每架无人机仅根据自己的局部观察进行决策，这符合实际通信限制。经典算法有 MADDPG、QMIX 等。
2. 模仿学习 (Imitation Learning)：在某些场景下，定义一个好的奖励函数非常困难。可以先让专家（人类飞手或一个最优的传统规划器）进行演示，然后让无人机通过模仿学习来学习专家的策略。
